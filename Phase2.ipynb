{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRSdZ8-JSta",
        "outputId": "f8201a71-b7b7-4e59-d0d5-890563ed954e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning from https://github.com/MrSalalad/Parallel-programming-final-project...\n",
            "Cloning into 'cuda_project'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 67 (delta 17), reused 29 (delta 8), pack-reused 27 (from 1)\u001b[K\n",
            "Receiving objects: 100% (67/67), 167.07 MiB | 31.56 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Updating files: 100% (24/24), done.\n",
            "/content/cuda_project/cuda_project/cuda_project/cuda_project/cuda_project/cuda_project/cuda_project/cuda_project\n",
            "✅ Data ready.\n",
            ".:\n",
            "build_gpu.bat  data\tmain.cpp     Makefile  src\n",
            "build_run.bat  include\tmain_gpu.cu  output\n",
            "\n",
            "./data:\n",
            "data_batch_1.bin  data_batch_3.bin  data_batch_5.bin\n",
            "data_batch_2.bin  data_batch_4.bin  test_batch.bin\n",
            "\n",
            "./include:\n",
            "autoencoder.h\t   gpu_autoencoder.h  layers.h\n",
            "cifar10_dataset.h  kernels.cuh\t      utils.h\n",
            "\n",
            "./output:\n",
            "model_cpu.bin\n",
            "\n",
            "./src:\n",
            "autoencoder.cpp  cifar10_dataset.cpp  gpu  layers.cpp  utils.cpp\n",
            "\n",
            "./src/gpu:\n",
            "gpu_autoencoder.cu  kernels_naive.cu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Clone code từ GitHub\n",
        "repo_url = \"https://github.com/MrSalalad/Parallel-programming-final-project\"\n",
        "\n",
        "if not os.path.exists(\"cuda_project\"):\n",
        "    print(f\"Cloning from {repo_url}...\")\n",
        "    !git clone $repo_url cuda_project\n",
        "else:\n",
        "    print(\"Repo đã tồn tại. Đang pull code mới nhất...\")\n",
        "    %cd cuda_project\n",
        "    !git pull\n",
        "    %cd ..\n",
        "\n",
        "# 2. Di chuyển vào thư mục dự án\n",
        "%cd cuda_project\n",
        "\n",
        "# 3. Tải & Setup Dataset (Vì dataset nặng thường không up lên git)\n",
        "# Code C++ của bạn đang trỏ tới \"./data\" nên ta cần tạo folder data và bỏ file bin vào đó\n",
        "if not os.path.exists(\"data/data_batch_1.bin\"):\n",
        "    print(\"Downloading & Setting up Data...\")\n",
        "    !mkdir -p data\n",
        "    !wget -q https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz -O cifar-10-binary.tar.gz\n",
        "    !tar -xzf cifar-10-binary.tar.gz\n",
        "\n",
        "    # Di chuyển file từ thư mục con ra ngoài (Flatten) để khớp với code \"./data\"\n",
        "    !mv cifar-10-batches-bin/* data/\n",
        "    !rm -rf cifar-10-batches-bin cifar-10-binary.tar.gz\n",
        "    print(\"✅ Data setup complete!\")\n",
        "else:\n",
        "    print(\"✅ Data ready.\")\n",
        "\n",
        "# Kiểm tra file\n",
        "!ls -R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o train_gpu \\\n",
        "main_gpu.cu \\\n",
        "src/gpu/gpu_autoencoder.cu \\\n",
        "src/gpu/kernels_naive.cu \\\n",
        "src/cifar10_dataset.cpp \\\n",
        "src/autoencoder.cpp \\\n",
        "src/layers.cpp \\\n",
        "-I./include \\\n",
        "-O3"
      ],
      "metadata": {
        "id": "Yex7W2l-Wzov"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./train_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUCuw-J5bQw6",
        "outputId": "6e7605c3-8a8b-46d1-9b1b-cbdc9ad5e6b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "       PHASE 2: GPU TRAINING (20 EPOCHS)          \n",
            "==================================================\n",
            "Loading training data...\n",
            "Initialized 50000 indices for shuffling.\n",
            "[CONFIG] Target Epochs: 20\n",
            "[INIT] Generating random weights...\n",
            "Initializing Autoencoder Weights...\n",
            "[INIT] Booting up GPU...\n",
            "Allocating GPU Memory (Batch Size: 64)...\n",
            "GPU Memory Allocated Successfully.\n",
            "[INFO] Training Started...\n",
            "Epoch  1/20 | Time: 279.70s | Avg Loss: 0.04501\n",
            "Epoch  2/20 | Time: 284.40s | Avg Loss: 0.02642\n",
            "Epoch  3/20 | Time: 284.21s | Avg Loss: 0.02215\n",
            "Epoch  4/20 | Time: 284.08s | Avg Loss: 0.01991\n",
            "Epoch  5/20 | Time: 284.14s | Avg Loss: 0.01846\n",
            "Epoch  6/20 | Time: 284.19s | Avg Loss: 0.01741\n",
            "Epoch  7/20 | Time: 284.04s | Avg Loss: 0.01661\n",
            "Epoch  8/20 | Time: 284.30s | Avg Loss: 0.01596\n",
            "Epoch  9/20 | Time: 284.32s | Avg Loss: 0.01542\n",
            "Epoch 10/20 | Time: 284.56s | Avg Loss: 0.01497\n",
            "Epoch 11/20 | Time: 284.41s | Avg Loss: 0.01458\n",
            "Epoch 12/20 | Time: 284.07s | Avg Loss: 0.01424\n",
            "Epoch 13/20 | Time: 284.01s | Avg Loss: 0.01394\n",
            "Epoch 14/20 | Time: 284.09s | Avg Loss: 0.01367\n",
            "Epoch 15/20 | Time: 284.08s | Avg Loss: 0.01343\n",
            "Epoch 16/20 | Time: 284.08s | Avg Loss: 0.01320\n",
            "Epoch 17/20 | Time: 284.13s | Avg Loss: 0.01300\n",
            "Epoch 18/20 | Time: 284.12s | Avg Loss: 0.01281\n",
            "Epoch 19/20 | Time: 284.01s | Avg Loss: 0.01263\n",
            "Epoch 20/20 | Time: 284.02s | Avg Loss: 0.01247\n",
            "\n",
            "==================================================\n",
            "               GPU RESULT REPORT                  \n",
            "==================================================\n",
            "1. PERFORMANCE:\n",
            "   - Total Time (20 Epochs): 5678.95178 seconds (94.64920 min)\n",
            "   - Avg Time per Epoch    : 283.94759 seconds\n",
            "\n",
            "2. MEMORY:\n",
            "   - VRAM Used : 292.94 MB\n",
            "   - VRAM Total: 15095.06 MB\n",
            "==================================================\n",
            "\n",
            "[SAVE] Model saved to ./output/model_gpu_phase2.bin\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}